{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "# from transformers import BertTokenizer, BertModel, AdamW\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bromwell High is a cartoon comedy. It ran at t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Homelessness (or Houselessness as George Carli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brilliant over-acting by Lesley Ann Warren. Be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is easily the most underrated film inn th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is not the typical Mel Brooks film. It wa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Bromwell High is a cartoon comedy. It ran at t...      1\n",
       "1  Homelessness (or Houselessness as George Carli...      1\n",
       "2  Brilliant over-acting by Lesley Ann Warren. Be...      1\n",
       "3  This is easily the most underrated film inn th...      1\n",
       "4  This is not the typical Mel Brooks film. It wa...      1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train set\n",
    "train_pos_folder = './data/aclImdb/train/pos'\n",
    "train_neg_folder = './data/aclImdb/train/neg'\n",
    "\n",
    "train_pos_sentences = [open(os.path.join(train_pos_folder, f)).read().strip() for f in os.listdir(train_pos_folder)]\n",
    "train_neg_sentences = [open(os.path.join(train_neg_folder, f)).read().strip() for f in os.listdir(train_neg_folder)]\n",
    "\n",
    "train_df = pd.DataFrame({\n",
    "    'text': train_pos_sentences + train_neg_sentences,\n",
    "    'label': [1] * len(train_pos_sentences) + [0] * len(train_neg_sentences)  # 1 for positive, 0 for negative\n",
    "})\n",
    "\n",
    "# Test set\n",
    "test_pos_folder = './data/aclImdb/test/pos'\n",
    "test_neg_folder = './data/aclImdb/test/neg'\n",
    "\n",
    "test_pos_sentences = [open(os.path.join(test_pos_folder, f)).read().strip() for f in os.listdir(test_pos_folder)]\n",
    "test_neg_sentences = [open(os.path.join(test_neg_folder, f)).read().strip() for f in os.listdir(test_neg_folder)]\n",
    "\n",
    "test_df = pd.DataFrame({\n",
    "    'text': test_pos_sentences + test_neg_sentences,\n",
    "    'label': [1] * len(test_pos_sentences) + [0] * len(test_neg_sentences)  # 1 for positive, 0 for negative\n",
    "})\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train NANs: 0 0\n",
      "test NANs: 0 0\n",
      "Check labels: [1 0] [1 0]\n",
      "max_words: 2470 2278\n"
     ]
    }
   ],
   "source": [
    "# Check for NaN values in columns\n",
    "print('train NANs:', train_df['text'].isna().sum(), train_df['label'].isna().sum())\n",
    "print('test NANs:', test_df['text'].isna().sum(), test_df['label'].isna().sum())\n",
    "\n",
    "# Check labels\n",
    "train_unique_values = train_df['label'].unique()\n",
    "test_unique_values = test_df['label'].unique()\n",
    "print('Check labels:', train_unique_values, test_unique_values)\n",
    "\n",
    "# Check max length\n",
    "train_max_words = train_df['text'].apply(lambda x: len(x.split())).max()\n",
    "test_max_words = test_df['text'].apply(lambda x: len(x.split())).max()\n",
    "print('max_words:', train_max_words, test_max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data.iloc[idx, 0]\n",
    "        label = self.data.iloc[idx, 1]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt')    # as pytorch tensors\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor([label], dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\downloads\\downloads_from_browser\\anaconda\\app\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Make torch DataLoader\n",
    "batch_size = 32\n",
    "\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = IMDBDataset(train_df, tokenizer, max_len=512)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = IMDBDataset(test_df, tokenizer, max_len=512)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "0 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "1 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "1 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "2 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "2 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "3 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "3 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "4 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "4 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "5 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "5 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "6 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "6 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "7 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "7 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "8 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "8 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "9 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "9 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "# Check tensors\n",
    "for i, (train_batch, test_batch) in enumerate(zip(train_loader, test_loader)):\n",
    "    if i == 10:\n",
    "        break\n",
    "    train_input_ids = train_batch['input_ids']\n",
    "    train_attention_mask = train_batch['attention_mask']\n",
    "    train_labels = train_batch['labels']\n",
    "    \n",
    "    test_input_ids = test_batch['input_ids']\n",
    "    test_attention_mask = test_batch['attention_mask']\n",
    "    test_labels = test_batch['labels']\n",
    "\n",
    "    print(i, train_input_ids.shape, train_attention_mask.shape, train_labels.shape)\n",
    "    print(i, test_input_ids.shape, test_attention_mask.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertBinaryClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_model_name='bert-base-uncased'):\n",
    "        super(BertBinaryClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(pretrained_model_name)\n",
    "        self.dropout = nn.Dropout(0.3)  # Dropout layer for regularization\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, 1)  # Output layer for binary classification\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output  # Get the pooled output (CLS token representation)\n",
    "        pooled_output = self.dropout(pooled_output)  # Apply dropout\n",
    "        logits = self.fc(pooled_output)  # Pass through the linear layer\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertBinaryClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train_model(model, dataloader, epochs=3):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    criterion = nn.BCEWithLogitsLoss()  # Use BCEWithLogitsLoss for binary classification\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        \n",
    "        for batch in dataloader:\n",
    "\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            print(input_ids.shape, attention_mask.shape, labels.shape)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            print(outputs.shape)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            preds = torch.sigmoid(outputs).detach().cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.detach().cpu().numpy())\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        accuracy = accuracy_score((np.array(all_labels) > 0.5).astype(int), (np.array(all_preds) > 0.5).astype(int))\n",
    "        \n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_model(model, train_loader)\n",
      "Cell \u001b[1;32mIn[24], line 19\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, dataloader, epochs)\u001b[0m\n\u001b[0;32m     15\u001b[0m all_preds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m---> 19\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     20\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     21\u001b[0m     labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
