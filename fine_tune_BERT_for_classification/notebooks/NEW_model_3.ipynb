{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device configuration\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "# torch.cuda.empty_cache()\n",
    "# torch.cuda.reset_max_memory_cached()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bromwell High is a cartoon comedy. It ran at t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Homelessness (or Houselessness as George Carli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brilliant over-acting by Lesley Ann Warren. Be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is easily the most underrated film inn th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is not the typical Mel Brooks film. It wa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Bromwell High is a cartoon comedy. It ran at t...      1\n",
       "1  Homelessness (or Houselessness as George Carli...      1\n",
       "2  Brilliant over-acting by Lesley Ann Warren. Be...      1\n",
       "3  This is easily the most underrated film inn th...      1\n",
       "4  This is not the typical Mel Brooks film. It wa...      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train set\n",
    "train_pos_folder = './data/aclImdb/train/pos'\n",
    "train_neg_folder = './data/aclImdb/train/neg'\n",
    "\n",
    "train_pos_sentences = [open(os.path.join(train_pos_folder, f)).read().strip() for f in os.listdir(train_pos_folder)]\n",
    "train_neg_sentences = [open(os.path.join(train_neg_folder, f)).read().strip() for f in os.listdir(train_neg_folder)]\n",
    "\n",
    "train_df = pd.DataFrame({\n",
    "    'text': train_pos_sentences + train_neg_sentences,\n",
    "    'label': [1] * len(train_pos_sentences) + [0] * len(train_neg_sentences)  # 1 for positive, 0 for negative\n",
    "})\n",
    "\n",
    "# Test set\n",
    "test_pos_folder = './data/aclImdb/test/pos'\n",
    "test_neg_folder = './data/aclImdb/test/neg'\n",
    "\n",
    "test_pos_sentences = [open(os.path.join(test_pos_folder, f)).read().strip() for f in os.listdir(test_pos_folder)]\n",
    "test_neg_sentences = [open(os.path.join(test_neg_folder, f)).read().strip() for f in os.listdir(test_neg_folder)]\n",
    "\n",
    "test_df = pd.DataFrame({\n",
    "    'text': test_pos_sentences + test_neg_sentences,\n",
    "    'label': [1] * len(test_pos_sentences) + [0] * len(test_neg_sentences)  # 1 for positive, 0 for negative\n",
    "})\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train NANs: 0 0\n",
      "test NANs: 0 0\n",
      "Check labels: [1 0] [1 0]\n",
      "max_words: 2470 2278\n"
     ]
    }
   ],
   "source": [
    "# Check for NaN values in columns\n",
    "print('train NANs:', train_df['text'].isna().sum(), train_df['label'].isna().sum())\n",
    "print('test NANs:', test_df['text'].isna().sum(), test_df['label'].isna().sum())\n",
    "\n",
    "# Check labels\n",
    "train_unique_values = train_df['label'].unique()\n",
    "test_unique_values = test_df['label'].unique()\n",
    "print('Check labels:', train_unique_values, test_unique_values)\n",
    "\n",
    "# Check max length\n",
    "train_max_words = train_df['text'].apply(lambda x: len(x.split())).max()\n",
    "test_max_words = test_df['text'].apply(lambda x: len(x.split())).max()\n",
    "print('max_words:', train_max_words, test_max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data.iloc[idx, 0]\n",
    "        label = self.data.iloc[idx, 1]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt')    # as pytorch tensors\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor([label], dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\downloads\\downloads_from_browser\\anaconda\\app\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Make torch DataLoader\n",
    "batch_size = 32\n",
    "\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = IMDBDataset(train_df, tokenizer, max_len=512)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = IMDBDataset(test_df, tokenizer, max_len=512)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "0 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "1 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "1 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "2 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "2 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "3 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "3 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "4 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "4 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "5 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "5 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "6 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "6 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "7 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "7 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "8 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "8 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "9 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "9 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "10 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "10 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "11 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "11 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "12 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "12 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "13 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "13 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "14 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "14 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "15 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "15 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "16 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "16 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "17 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "17 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "18 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "18 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "19 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "19 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "20 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "20 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "21 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "21 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "22 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "22 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "23 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "23 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "24 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "24 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "25 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "25 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "26 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "26 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "27 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "27 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "28 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "28 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "29 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n",
      "29 torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "# Check tensors\n",
    "for i, (train_batch, test_batch) in enumerate(zip(train_loader, test_loader)):\n",
    "    if i == 30:\n",
    "        break\n",
    "    train_input_ids = train_batch['input_ids']\n",
    "    train_attention_mask = train_batch['attention_mask']\n",
    "    train_labels = train_batch['labels']\n",
    "    \n",
    "    test_input_ids = test_batch['input_ids']\n",
    "    test_attention_mask = test_batch['attention_mask']\n",
    "    test_labels = test_batch['labels']\n",
    "\n",
    "    print(i, train_input_ids.shape, train_attention_mask.shape, train_labels.shape)\n",
    "    print(i, test_input_ids.shape, test_attention_mask.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparmaters\n",
    "num_epochs = 3\n",
    "learning_rate = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the BertForSentenceClassification model\n",
    "class BertForSentenceClassification(nn.Module):\n",
    "    def __init__(self, bert_model):\n",
    "        super(BertForSentenceClassification, self).__init__()\n",
    "        self.bert_model = bert_model\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(self.bert_model.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert_model(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        outputs = self.classifier(pooled_output)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSentenceClassification(\n",
       "  (bert_model): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply and check the model\n",
    "\n",
    "# Load pre-trained BERT model\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Create an instance of the custom model\n",
    "model = BertForSentenceClassification(bert_model)\n",
    "model.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\downloads\\downloads_from_browser\\anaconda\\app\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "# Basic check\n",
    "input_ids = torch.randint(0, 100, (32, 512)).to(device)  # random input IDs\n",
    "attention_mask = torch.ones((32, 512)).to(device)  # random attention mask\n",
    "output = model(input_ids, attention_mask)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer and loss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# # Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     17\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(input_ids, attention_mask)\n\u001b[1;32m---> 18\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     20\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32md:\\downloads\\downloads_from_browser\\anaconda\\app\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\downloads\\downloads_from_browser\\anaconda\\app\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\downloads\\downloads_from_browser\\anaconda\\app\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:618\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 618\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mbinary_cross_entropy(\u001b[38;5;28minput\u001b[39m, target, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction)\n",
      "File \u001b[1;32md:\\downloads\\downloads_from_browser\\anaconda\\app\\Lib\\site-packages\\torch\\nn\\functional.py:3154\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3151\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m   3152\u001b[0m     weight \u001b[38;5;241m=\u001b[39m weight\u001b[38;5;241m.\u001b[39mexpand(new_size)\n\u001b[1;32m-> 3154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mbinary_cross_entropy(\u001b[38;5;28minput\u001b[39m, target, weight, reduction_enum)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for epoch in range(1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        print(f\"batch: {i}\", end=\"\\r\")\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        print(input_ids.shape, attention_mask.shape, labels.shape)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    # Append the current loss to the list\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss curve\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_loss = 0\n",
    "test_metrics = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device=device)\n",
    "        attention_mask = batch['attention_mask'].to(device=device)\n",
    "        labels = batch['labels'].to(device=device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Calculate metrics (e.g., accuracy, F1-score)\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        accuracy = (preds == labels).sum().item() / len(labels)\n",
    "        test_metrics.append(accuracy)\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "test_metrics = np.mean(test_metrics)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "print(f'Test Accuracy: {test_metrics:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
