{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pydicom\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pydicom\n",
    "import cv2\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# To resize images\n",
    "def resize_image(image_path):\n",
    "    dicom = pydicom.dcmread(image_path)\n",
    "\n",
    "    # Convert the DCM image to a numpy array\n",
    "    image_array = dicom.pixel_array\n",
    "\n",
    "    # Resize the image using OpenCV\n",
    "    resized_image = cv2.resize(image_array, (224, 224))\n",
    "\n",
    "    return resized_image\n",
    "\n",
    "# To stack on z-axis and pad tenors\n",
    "def pad_to_3d(unpad_dict, max_channels=6):\n",
    "\n",
    "    grouped_arrays = {}\n",
    "    for key, value in unpad_dict.items():\n",
    "        if key[0] not in grouped_arrays:\n",
    "            grouped_arrays[key[0]] = [value]\n",
    "        else:\n",
    "            grouped_arrays[key[0]].append(value)\n",
    "\n",
    "    stacked_arrays = {}\n",
    "    for key, values in grouped_arrays.items():\n",
    "        stacked_arrays[key] = np.stack(values, axis=0)\n",
    "\n",
    "    stacked_tensors = {}\n",
    "    for key, value in stacked_arrays.items():\n",
    "        stacked_tensors[key] = torch.from_numpy(value)\n",
    "\n",
    "    for key, tensor in stacked_tensors.items():\n",
    "        if max_channels-tensor.shape[1] > 0:\n",
    "            pad_size = max_channels - tensor.shape[1]\n",
    "            stacked_tensors[key] = F.pad(tensor, (0, 0, 0, 0, pad_size, 0))\n",
    "\n",
    "    return stacked_tensors\n",
    "\n",
    "\n",
    "\n",
    "# To prepare for CNN\n",
    "def prepare_for_cnn(root_directory, targets_df, study_id, study_id_dir, batch_size):\n",
    "    \n",
    "    # Prepare features\n",
    "    image_dict = {}\n",
    "    max_shape = tuple([43008, 224])\n",
    "    for series_id in study_id_dir:\n",
    "        series_id_dir = os.listdir(f'{root_directory}/{study_id}/{series_id}')\n",
    "        # Initialize list to store image arrays for the series\n",
    "        image_arrays = []\n",
    "        counter += 1\n",
    "        # Iterate over DICOM files in the series folder\n",
    "        for idx in range(1, len(series_id_dir)+1):\n",
    "            image_path = f'{root_directory}/{study_id}/{series_id}/{idx}.dcm'\n",
    "            resized_image = resize_image(image_path)\n",
    "            # Append resized_image array to the list\n",
    "            image_arrays.append(resized_image)\n",
    "\n",
    "        # Vertically stack DCM images\n",
    "        stacked_images = np.vstack(image_arrays)\n",
    "        # Store stacked images as a NumPy array\n",
    "        np_array = np.array(stacked_images)\n",
    "\n",
    "        # Pad them\n",
    "        padding = max_shape[0] - np_array.shape[0]\n",
    "        if padding > 0:\n",
    "            padding_shape = ((0, padding), (0, 0))\n",
    "            padded_np_array = np.pad(np_array, padding_shape, mode='constant', constant_values=0) # 0 is black (If I'm not mistaken)\n",
    "            # Store image arrays in the dictionary with (study_id, series_id) tuple as key\n",
    "            image_dict[(study_id, series_id)] = padded_np_array\n",
    "            print(counter, series_id, padded_np_array.shape)\n",
    "        else:\n",
    "            image_dict[(study_id, series_id)] = np_array\n",
    "            print(counter, series_id, np_array.shape)\n",
    "    stacked_tensors = pad_to_3d(image_dict)\n",
    "\n",
    "    \n",
    "    # Prepare targets\n",
    "    targets_tensors = {}\n",
    "    for key, _ in stacked_tensors.items():\n",
    "        target = targets_df[targets_df['study_id'] == int(key)]\n",
    "        transposed_df = target.iloc[:, 1:].T\n",
    "        one_hot_array = []\n",
    "        for _, row in transposed_df.iterrows():\n",
    "            if row.values == 'Normal/Mild':\n",
    "                one_hot_array.append([1, 0, 0])\n",
    "            elif row.values == 'Moderate':\n",
    "                one_hot_array.append([0, 1, 0])\n",
    "            elif row.values == 'Severe':\n",
    "                one_hot_array.append([0, 0, 1])\n",
    "        targets_tensors[key] = torch.tensor(one_hot_array)\n",
    "    \n",
    "\n",
    "    # Convert all tensors to float32\n",
    "    feature_tensors = [tensor.float() for tensor in stacked_tensors.values()]\n",
    "    target_tensors = [tensor.float() for tensor in targets_tensors.values()]\n",
    "\n",
    "    # Stack the tensors\n",
    "    X_train = torch.stack(feature_tensors)\n",
    "    y_train = torch.stack(target_tensors)\n",
    "\n",
    "\n",
    "    # Make torch DataLoader\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Check DataLoader batches\n",
    "    # for i, train_batch in enumerate(train_loader):\n",
    "    #     x_train_batch, y_train_batch = train_batch\n",
    "    #     print(f'train tensor {i+1}', '|', x_train_batch.shape, '|', y_train_batch.shape)\n",
    "\n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_directory = '../train_images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_id_list = os.listdir(root_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 10\n",
    "for i in range(0, len(study_id_list), chunk_size):\n",
    "    study_id_chunk = study_id_list[i:i + chunk_size]\n",
    "    for study_id in study_id_chunk:\n",
    "        if study_id not in exclude_array:\n",
    "            study_id_dir = os.listdir(f'{root_directory}/{study_id}')\n",
    "            features, targets = prepare_for_cnn(root_directory, study_id, study_id_dir, targets_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
