{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pydicom\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(image_path):\n",
    "    dicom = pydicom.dcmread(image_path)\n",
    "\n",
    "    # Convert the DCM image to a numpy array\n",
    "    image_array = dicom.pixel_array\n",
    "\n",
    "    # Resize the image using OpenCV\n",
    "    resized_image = cv2.resize(image_array, (224, 224))\n",
    "\n",
    "    return resized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142859125 (8064, 224)\n",
      "2073726394 (10304, 224)\n",
      "2399638375 (4256, 224)\n",
      "3491739931 (4256, 224)\n",
      "1224932122 (10080, 224)\n",
      "2231042680 (4032, 224)\n",
      "3543553307 (4032, 224)\n",
      "1212326388 (3360, 224)\n",
      "1638921810 (3360, 224)\n",
      "3800798510 (6048, 224)\n",
      "403244853 (6272, 224)\n",
      "1539051863 (3808, 224)\n",
      "2500166693 (6048, 224)\n",
      "2677627096 (3808, 224)\n",
      "3687121182 (12992, 224)\n",
      "3753885158 (4032, 224)\n",
      "434280813 (4032, 224)\n",
      "1679014482 (10080, 224)\n",
      "226564374 (3808, 224)\n",
      "2528347280 (3808, 224)\n",
      "307069509 (5600, 224)\n",
      "1152175603 (5152, 224)\n",
      "1676821058 (5152, 224)\n",
      "2261718442 (6720, 224)\n",
      "231278500 (8960, 224)\n",
      "1379151387 (3360, 224)\n",
      "1847558962 (4928, 224)\n",
      "758801267 (3360, 224)\n",
      "1054713880 (3360, 224)\n",
      "2448190387 (9632, 224)\n",
      "702807833 (3360, 224)\n",
      "3201256954 (12096, 224)\n",
      "3486248476 (3808, 224)\n",
      "3666319702 (3808, 224)\n",
      "132939515 (3808, 224)\n",
      "1951927562 (5152, 224)\n",
      "3219733239 (3808, 224)\n",
      "1570286759 (3360, 224)\n",
      "2406919186 (4704, 224)\n",
      "481125819 (3360, 224)\n"
     ]
    }
   ],
   "source": [
    "# Specify the path to the main directory containing patient folders\n",
    "main_directory = './small_train_images'\n",
    "\n",
    "# Dictionary to store image data\n",
    "image_data = {}\n",
    "\n",
    "# Iterate over study_id folders\n",
    "for study_id in os.listdir(main_directory):\n",
    "    study_id_dir = os.listdir(f'./small_train_images/{study_id}')\n",
    "    \n",
    "    # Iterate over series folders for each patient\n",
    "    for series_id in study_id_dir:\n",
    "        series_id_dir = os.listdir(f'./small_train_images/{study_id}/{series_id}')\n",
    "        # Initialize list to store image arrays for the series\n",
    "        image_arrays = []\n",
    "        \n",
    "        # Iterate over DICOM files in the series folder\n",
    "        for instance in series_id_dir:\n",
    "            image_path = f'./small_train_images/{study_id}/{series_id}/{instance}'\n",
    "            resized_image = resize_image(image_path)\n",
    "\n",
    "            # Append resized_image array to the list\n",
    "            image_arrays.append(resized_image)\n",
    "        \n",
    "        # Vertically stack DCM images\n",
    "        stacked_images = np.vstack(image_arrays)\n",
    "        # Store stacked images as a NumPy array\n",
    "        np_array = np.array(stacked_images)\n",
    "        print(series_id, np_array.shape)\n",
    "\n",
    "        # Store image arrays in the dictionary with (study_id, series_id) tuple as key\n",
    "        image_data[(study_id, series_id)] = np_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (3360, 224)\n"
     ]
    }
   ],
   "source": [
    "# Check shapes\n",
    "example_study_id = '4003253'\n",
    "example_series_id = '702807833'\n",
    "if (example_study_id, example_series_id) in image_data:\n",
    "    image = image_data[(example_study_id, example_series_id)]\n",
    "    print(f\"Image shape:\", image.shape)\n",
    "else:\n",
    "    print(\"No images found for the specified (study_id, series_id) tuple.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('10728036', '142859125') (12992, 224)\n",
      "('10728036', '2073726394') (12992, 224)\n",
      "('10728036', '2399638375') (12992, 224)\n",
      "('10728036', '3491739931') (12992, 224)\n",
      "('11340341', '1224932122') (12992, 224)\n",
      "('11340341', '2231042680') (12992, 224)\n",
      "('11340341', '3543553307') (12992, 224)\n",
      "('11943292', '1212326388') (12992, 224)\n",
      "('11943292', '1638921810') (12992, 224)\n",
      "('11943292', '3800798510') (12992, 224)\n",
      "('11943292', '403244853') (12992, 224)\n",
      "('13317052', '1539051863') (12992, 224)\n",
      "('13317052', '2500166693') (12992, 224)\n",
      "('13317052', '2677627096') (12992, 224)\n",
      "('22191399', '3687121182') (12992, 224)\n",
      "('22191399', '3753885158') (12992, 224)\n",
      "('22191399', '434280813') (12992, 224)\n",
      "('26342422', '1679014482') (12992, 224)\n",
      "('26342422', '226564374') (12992, 224)\n",
      "('26342422', '2528347280') (12992, 224)\n",
      "('26342422', '307069509') (12992, 224)\n",
      "('29931867', '1152175603') (12992, 224)\n",
      "('29931867', '1676821058') (12992, 224)\n",
      "('29931867', '2261718442') (12992, 224)\n",
      "('29931867', '231278500') (12992, 224)\n",
      "('33736057', '1379151387') (12992, 224)\n",
      "('33736057', '1847558962') (12992, 224)\n",
      "('33736057', '758801267') (12992, 224)\n",
      "('4003253', '1054713880') (12992, 224)\n",
      "('4003253', '2448190387') (12992, 224)\n",
      "('4003253', '702807833') (12992, 224)\n",
      "('4646740', '3201256954') (12992, 224)\n",
      "('4646740', '3486248476') (12992, 224)\n",
      "('4646740', '3666319702') (12992, 224)\n",
      "('7143189', '132939515') (12992, 224)\n",
      "('7143189', '1951927562') (12992, 224)\n",
      "('7143189', '3219733239') (12992, 224)\n",
      "('8785691', '1570286759') (12992, 224)\n",
      "('8785691', '2406919186') (12992, 224)\n",
      "('8785691', '481125819') (12992, 224)\n"
     ]
    }
   ],
   "source": [
    "# Pad arrays\n",
    "\n",
    "# Find the maximum shape among all numpy arrays\n",
    "max_shape = max([np_array.shape for np_array in image_data.values()], key=lambda x: x[0])\n",
    "\n",
    "for key in image_data:\n",
    "    np_array = image_data[key]\n",
    "    padding = max_shape[0] - np_array.shape[0]\n",
    "    if padding > 0:\n",
    "        padding_shape = ((0, padding), (0, 0))\n",
    "        padded_np_array = np.pad(np_array, padding_shape, mode='constant', constant_values=0) # 0 is black (If I'm not mistaken)\n",
    "        image_data[key] = padded_np_array\n",
    "\n",
    "# Print the shapes of padded numpy arrays\n",
    "for key in image_data:\n",
    "    print(key, image_data[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked arrays for key 10728036: torch.Size([4, 12992, 224])\n",
      "Stacked arrays for key 11340341: torch.Size([3, 12992, 224])\n",
      "Stacked arrays for key 11943292: torch.Size([4, 12992, 224])\n",
      "Stacked arrays for key 13317052: torch.Size([3, 12992, 224])\n",
      "Stacked arrays for key 22191399: torch.Size([3, 12992, 224])\n",
      "Stacked arrays for key 26342422: torch.Size([4, 12992, 224])\n",
      "Stacked arrays for key 29931867: torch.Size([4, 12992, 224])\n",
      "Stacked arrays for key 33736057: torch.Size([3, 12992, 224])\n",
      "Stacked arrays for key 4003253: torch.Size([3, 12992, 224])\n",
      "Stacked arrays for key 4646740: torch.Size([3, 12992, 224])\n",
      "Stacked arrays for key 7143189: torch.Size([3, 12992, 224])\n",
      "Stacked arrays for key 8785691: torch.Size([3, 12992, 224])\n"
     ]
    }
   ],
   "source": [
    "# Make 3D arrays\n",
    "\n",
    "# Group arrays by the first tuple values\n",
    "grouped_arrays = {}\n",
    "for key, value in image_data.items():\n",
    "    if key[0] not in grouped_arrays:\n",
    "        grouped_arrays[key[0]] = [value]\n",
    "    else:\n",
    "        grouped_arrays[key[0]].append(value)\n",
    "\n",
    "# Stack arrays with the same first tuple values into a 3D array\n",
    "stacked_arrays = {}\n",
    "for key, values in grouped_arrays.items():\n",
    "    stacked_arrays[key] = np.stack(values, axis=0)\n",
    "\n",
    "stacked_tensors = {}\n",
    "for key, value in stacked_arrays.items():\n",
    "    stacked_tensors[key] = torch.from_numpy(value)\n",
    "\n",
    "# Check the stacked arrays\n",
    "for key, value in stacked_tensors.items():\n",
    "    print(f\"Stacked arrays for key {key}: {value.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the maximum shape\n",
    "max_shape = tuple(max(arr.shape[i] for arr in stacked_tensors.values()) for i in range(3))\n",
    "\n",
    "# Pad each tensor to the maximum shape\n",
    "for key, tensor in stacked_tensors.items():\n",
    "    pad_shape = [max_shape[i] - tensor.shape[i] for i in range(3)]\n",
    "    stacked_tensors[key] = F.pad(tensor, (0, pad_shape[2], 0, pad_shape[1], 0, pad_shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: 10728036, New Shape: torch.Size([4, 12992, 224])\n",
      "Key: 11340341, New Shape: torch.Size([4, 12992, 224])\n",
      "Key: 11943292, New Shape: torch.Size([4, 12992, 224])\n",
      "Key: 13317052, New Shape: torch.Size([4, 12992, 224])\n",
      "Key: 22191399, New Shape: torch.Size([4, 12992, 224])\n",
      "Key: 26342422, New Shape: torch.Size([4, 12992, 224])\n",
      "Key: 29931867, New Shape: torch.Size([4, 12992, 224])\n",
      "Key: 33736057, New Shape: torch.Size([4, 12992, 224])\n",
      "Key: 4003253, New Shape: torch.Size([4, 12992, 224])\n",
      "Key: 4646740, New Shape: torch.Size([4, 12992, 224])\n",
      "Key: 7143189, New Shape: torch.Size([4, 12992, 224])\n",
      "Key: 8785691, New Shape: torch.Size([4, 12992, 224])\n"
     ]
    }
   ],
   "source": [
    "# Print the new shapes\n",
    "for key, arr in stacked_tensors.items():\n",
    "    print(f\"Key: {key}, New Shape: {arr.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add targets\n",
    "targets_df = pd.read_csv('./train.csv')\n",
    "\n",
    "mapping = {\n",
    "    'normal/mild': 1,\n",
    "    'moderate': 2,\n",
    "    'severe': 3\n",
    "}\n",
    "def standardize_and_map(column, mapping):\n",
    "    return column.str.lower().str.strip().map(mapping)\n",
    "\n",
    "# List of columns to apply the transformation\n",
    "columns_to_transform = ['spinal_canal_stenosis_l1_l2',\n",
    "       'spinal_canal_stenosis_l2_l3', 'spinal_canal_stenosis_l3_l4',\n",
    "       'spinal_canal_stenosis_l4_l5', 'spinal_canal_stenosis_l5_s1',\n",
    "       'left_neural_foraminal_narrowing_l1_l2',\n",
    "       'left_neural_foraminal_narrowing_l2_l3',\n",
    "       'left_neural_foraminal_narrowing_l3_l4',\n",
    "       'left_neural_foraminal_narrowing_l4_l5',\n",
    "       'left_neural_foraminal_narrowing_l5_s1',\n",
    "       'right_neural_foraminal_narrowing_l1_l2',\n",
    "       'right_neural_foraminal_narrowing_l2_l3',\n",
    "       'right_neural_foraminal_narrowing_l3_l4',\n",
    "       'right_neural_foraminal_narrowing_l4_l5',\n",
    "       'right_neural_foraminal_narrowing_l5_s1',\n",
    "       'left_subarticular_stenosis_l1_l2', 'left_subarticular_stenosis_l2_l3',\n",
    "       'left_subarticular_stenosis_l3_l4', 'left_subarticular_stenosis_l4_l5',\n",
    "       'left_subarticular_stenosis_l5_s1', 'right_subarticular_stenosis_l1_l2',\n",
    "       'right_subarticular_stenosis_l2_l3',\n",
    "       'right_subarticular_stenosis_l3_l4',\n",
    "       'right_subarticular_stenosis_l4_l5',\n",
    "       'right_subarticular_stenosis_l5_s1']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the specified columns\n",
    "for column in columns_to_transform:\n",
    "    targets_df[column] = standardize_and_map(targets_df[column], mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>study_id</th>\n",
       "      <th>spinal_canal_stenosis_l1_l2</th>\n",
       "      <th>spinal_canal_stenosis_l2_l3</th>\n",
       "      <th>spinal_canal_stenosis_l3_l4</th>\n",
       "      <th>spinal_canal_stenosis_l4_l5</th>\n",
       "      <th>spinal_canal_stenosis_l5_s1</th>\n",
       "      <th>left_neural_foraminal_narrowing_l1_l2</th>\n",
       "      <th>left_neural_foraminal_narrowing_l2_l3</th>\n",
       "      <th>left_neural_foraminal_narrowing_l3_l4</th>\n",
       "      <th>left_neural_foraminal_narrowing_l4_l5</th>\n",
       "      <th>...</th>\n",
       "      <th>left_subarticular_stenosis_l1_l2</th>\n",
       "      <th>left_subarticular_stenosis_l2_l3</th>\n",
       "      <th>left_subarticular_stenosis_l3_l4</th>\n",
       "      <th>left_subarticular_stenosis_l4_l5</th>\n",
       "      <th>left_subarticular_stenosis_l5_s1</th>\n",
       "      <th>right_subarticular_stenosis_l1_l2</th>\n",
       "      <th>right_subarticular_stenosis_l2_l3</th>\n",
       "      <th>right_subarticular_stenosis_l3_l4</th>\n",
       "      <th>right_subarticular_stenosis_l4_l5</th>\n",
       "      <th>right_subarticular_stenosis_l5_s1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4003253</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   study_id  spinal_canal_stenosis_l1_l2  spinal_canal_stenosis_l2_l3  \\\n",
       "0   4003253                          1.0                          1.0   \n",
       "\n",
       "   spinal_canal_stenosis_l3_l4  spinal_canal_stenosis_l4_l5  \\\n",
       "0                          1.0                          1.0   \n",
       "\n",
       "   spinal_canal_stenosis_l5_s1  left_neural_foraminal_narrowing_l1_l2  \\\n",
       "0                          1.0                                    1.0   \n",
       "\n",
       "   left_neural_foraminal_narrowing_l2_l3  \\\n",
       "0                                    1.0   \n",
       "\n",
       "   left_neural_foraminal_narrowing_l3_l4  \\\n",
       "0                                    1.0   \n",
       "\n",
       "   left_neural_foraminal_narrowing_l4_l5  ...  \\\n",
       "0                                    2.0  ...   \n",
       "\n",
       "   left_subarticular_stenosis_l1_l2  left_subarticular_stenosis_l2_l3  \\\n",
       "0                               1.0                               1.0   \n",
       "\n",
       "   left_subarticular_stenosis_l3_l4  left_subarticular_stenosis_l4_l5  \\\n",
       "0                               1.0                               2.0   \n",
       "\n",
       "   left_subarticular_stenosis_l5_s1  right_subarticular_stenosis_l1_l2  \\\n",
       "0                               1.0                                1.0   \n",
       "\n",
       "   right_subarticular_stenosis_l2_l3  right_subarticular_stenosis_l3_l4  \\\n",
       "0                                1.0                                1.0   \n",
       "\n",
       "   right_subarticular_stenosis_l4_l5  right_subarticular_stenosis_l5_s1  \n",
       "0                                1.0                                1.0  \n",
       "\n",
       "[1 rows x 26 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = targets_df[targets_df['study_id'] == 4003253]\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store targets in a dict as torch tensors and rashape them\n",
    "\n",
    "targets_tensors = {}\n",
    "\n",
    "for key, arr in stacked_tensors.items():\n",
    "    target = targets_df[targets_df['study_id'] == int(key)]\n",
    "    target_study_id = target.iloc[:, :1]\n",
    "    target_values_int = np.array(target.iloc[:, 1:]).flatten().astype(int) - 1\n",
    "\n",
    "    # Convert the target array to one-hot encoding with three classes\n",
    "    num_classes = 3\n",
    "    one_hot_targets = np.eye(num_classes)[target_values_int]\n",
    "\n",
    "    # Reshape the one-hot encoded target array to match the desired output shape (25, 3)\n",
    "    reshaped_targets = one_hot_targets.reshape(-1, num_classes)\n",
    "\n",
    "    # Convert the numpy array to a torch tensor\n",
    "    tensor_targets = torch.from_numpy(reshaped_targets)\n",
    "\n",
    "    targets_tensors[target_study_id.iloc[0, 0]] = tensor_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: 10728036, New Shape: torch.Size([25, 3])\n",
      "Key: 11340341, New Shape: torch.Size([25, 3])\n",
      "Key: 11943292, New Shape: torch.Size([25, 3])\n",
      "Key: 13317052, New Shape: torch.Size([25, 3])\n",
      "Key: 22191399, New Shape: torch.Size([25, 3])\n",
      "Key: 26342422, New Shape: torch.Size([25, 3])\n",
      "Key: 29931867, New Shape: torch.Size([25, 3])\n",
      "Key: 33736057, New Shape: torch.Size([25, 3])\n",
      "Key: 4003253, New Shape: torch.Size([25, 3])\n",
      "Key: 4646740, New Shape: torch.Size([25, 3])\n",
      "Key: 7143189, New Shape: torch.Size([25, 3])\n",
      "Key: 8785691, New Shape: torch.Size([25, 3])\n"
     ]
    }
   ],
   "source": [
    "# Print the new shapes\n",
    "for key, arr in targets_tensors.items():\n",
    "    print(f\"Key: {key}, New Shape: {arr.shape}\")\n",
    "    # if key == 4003253:\n",
    "    #     print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: 10728036, New Shape: torch.Size([4, 12992, 224]) | Key: 10728036, New Shape: torch.Size([25, 3])\n",
      "Key: 11340341, New Shape: torch.Size([4, 12992, 224]) | Key: 11340341, New Shape: torch.Size([25, 3])\n",
      "Key: 11943292, New Shape: torch.Size([4, 12992, 224]) | Key: 11943292, New Shape: torch.Size([25, 3])\n",
      "Key: 13317052, New Shape: torch.Size([4, 12992, 224]) | Key: 13317052, New Shape: torch.Size([25, 3])\n",
      "Key: 22191399, New Shape: torch.Size([4, 12992, 224]) | Key: 22191399, New Shape: torch.Size([25, 3])\n",
      "Key: 26342422, New Shape: torch.Size([4, 12992, 224]) | Key: 26342422, New Shape: torch.Size([25, 3])\n",
      "Key: 29931867, New Shape: torch.Size([4, 12992, 224]) | Key: 29931867, New Shape: torch.Size([25, 3])\n",
      "Key: 33736057, New Shape: torch.Size([4, 12992, 224]) | Key: 33736057, New Shape: torch.Size([25, 3])\n",
      "Key: 4003253, New Shape: torch.Size([4, 12992, 224]) | Key: 4003253, New Shape: torch.Size([25, 3])\n",
      "Key: 4646740, New Shape: torch.Size([4, 12992, 224]) | Key: 4646740, New Shape: torch.Size([25, 3])\n",
      "Key: 7143189, New Shape: torch.Size([4, 12992, 224]) | Key: 7143189, New Shape: torch.Size([25, 3])\n",
      "Key: 8785691, New Shape: torch.Size([4, 12992, 224]) | Key: 8785691, New Shape: torch.Size([25, 3])\n"
     ]
    }
   ],
   "source": [
    "for ((key, feature),(key2, targets)) in zip(stacked_tensors.items(), targets_tensors.items()):\n",
    "    print(f\"Key: {key}, New Shape: {feature.shape} | Key: {key2}, New Shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 4, 12992, 224]) torch.Size([12, 25, 3])\n"
     ]
    }
   ],
   "source": [
    "# Convert all tensors to float32\n",
    "feature_tensorstest = [tensor.float() for tensor in stacked_tensors.values()]\n",
    "target_tensorstest = [tensor.float() for tensor in targets_tensors.values()]\n",
    "\n",
    "# Stack the tensors\n",
    "X_train = torch.stack(feature_tensorstest)\n",
    "y_train = torch.stack(target_tensorstest)\n",
    "\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make torch DataLoader\n",
    "batch_size = 2\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train tensor 1 | torch.Size([2, 4, 12992, 224]) | torch.Size([2, 25, 3])\n",
      "train tensor 2 | torch.Size([2, 4, 12992, 224]) | torch.Size([2, 25, 3])\n",
      "train tensor 3 | torch.Size([2, 4, 12992, 224]) | torch.Size([2, 25, 3])\n",
      "train tensor 4 | torch.Size([2, 4, 12992, 224]) | torch.Size([2, 25, 3])\n",
      "train tensor 5 | torch.Size([2, 4, 12992, 224]) | torch.Size([2, 25, 3])\n",
      "train tensor 6 | torch.Size([2, 4, 12992, 224]) | torch.Size([2, 25, 3])\n"
     ]
    }
   ],
   "source": [
    "# Check DataLoader batches\n",
    "for i, train_batch in enumerate(train_loader):\n",
    "    x_train_batch, y_train_batch = train_batch\n",
    "    print(f'train tensor {i+1}', '|', x_train_batch.shape, '|', y_train_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        # self.layer1 = nn.Sequential(\n",
    "        #     nn.Conv2d(in_channels=4, out_channels=16, kernel_size=5, stride=1, padding=2),\n",
    "        #     nn.MaxPool2d(kernel_size=(8, 2), stride=(8, 2)))  # Large pooling to reduce height quickly\n",
    "        # self.layer2 = nn.Sequential(\n",
    "        #     nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2),\n",
    "        #     nn.MaxPool2d(kernel_size=(4, 2), stride=(4, 2)))  # Smaller pooling\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=4, out_channels=16, kernel_size=5, stride=1, padding=2)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(8, 2), stride=(8, 2))  # Large pooling to reduce height quickly\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(4, 2), stride=(4, 2))  # Smaller pooling\n",
    "\n",
    "        self.fc1 = nn.Linear(32*406*56, 25*3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        # x = self.fc1(x)\n",
    "        # x = x.view(x.size(0), -1)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = x.reshape(x.shape[0], 25, 3) # added this\n",
    "        # x = F.softmax(x, dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNModel(\n",
      "  (conv1): Conv2d(4, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (pool): MaxPool2d(kernel_size=(8, 2), stride=(8, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (pool2): MaxPool2d(kernel_size=(4, 2), stride=(4, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=727552, out_features=75, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Apply and check the model\n",
    "model = CNNModel()\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 25, 3])\n"
     ]
    }
   ],
   "source": [
    "# Basic check\n",
    "x = torch.randn(2, 4, 12992, 224)\n",
    "print(model(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 28.8335\n",
      "Epoch [2/10], Loss: 26.7903\n",
      "Epoch [3/10], Loss: 26.7774\n",
      "Epoch [4/10], Loss: 26.7672\n",
      "Epoch [5/10], Loss: 26.7583\n",
      "Epoch [6/10], Loss: 26.7512\n",
      "Epoch [7/10], Loss: 26.7457\n",
      "Epoch [8/10], Loss: 26.7414\n",
      "Epoch [9/10], Loss: 26.7379\n",
      "Epoch [10/10], Loss: 26.7348\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    for features, targets in train_loader:\n",
    "\n",
    "        # Move data to GPU\n",
    "        features, targets = features.to(device), targets.to(device)\n",
    "\n",
    "        # Ensure the target tensor is a 0D or 1D tensor\n",
    "        # targets = targets.view(2, -1)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Append the current loss to the list\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    # Print the loss after each epoch\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_tensor):\n",
    "\n",
    "    # Convert input tensor to float\n",
    "    input_tensor = input_tensor.float()\n",
    "\n",
    "    # Ensure the model is in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Disable gradient computation for inference\n",
    "    with torch.no_grad():\n",
    "        # Forward pass through the model\n",
    "        output = model(input_tensor)\n",
    "        \n",
    "        # Convert the model output to probabilities using softmax\n",
    "        probabilities = F.softmax(output, dim=1)\n",
    "        \n",
    "        return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3485741709 (4032, 224)\n",
      "3742439825 (10976, 224)\n",
      "507422603 (4032, 224)\n"
     ]
    }
   ],
   "source": [
    "# Try to predict stock prices for Apple company\n",
    "predict_dirs = os.listdir('./train_images/74294498')\n",
    "\n",
    "# Dictionary to store image data\n",
    "test_image_data = {}\n",
    "\n",
    "for series_id in predict_dirs:\n",
    "    series_id_dir = os.listdir(f'./train_images/74294498/{series_id}')\n",
    "    # Initialize list to store image arrays for the series\n",
    "    image_arrays = []\n",
    "    \n",
    "    # Iterate over DICOM files in the series folder\n",
    "    for instance in series_id_dir:\n",
    "        image_path = f'./train_images/74294498/{series_id}/{instance}'\n",
    "        resized_image = resize_image(image_path)\n",
    "\n",
    "        # Append resized_image array to the list\n",
    "        image_arrays.append(resized_image)\n",
    "    \n",
    "    # Vertically stack DCM images\n",
    "    stacked_images = np.vstack(image_arrays)\n",
    "    # Store stacked images as a NumPy array\n",
    "    np_array = np.array(stacked_images)\n",
    "    print(series_id, np_array.shape)\n",
    "\n",
    "    # Store image arrays in the dictionary with (study_id, series_id) tuple as key\n",
    "    test_image_data[(study_id, series_id)] = np_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('8785691', '3485741709') (12992, 224)\n",
      "('8785691', '3742439825') (12992, 224)\n",
      "('8785691', '507422603') (12992, 224)\n"
     ]
    }
   ],
   "source": [
    "# Pad arrays\n",
    "\n",
    "# Find the maximum shape among all numpy arrays\n",
    "# max_shape = max([np_array.shape for np_array in test_image_data.values()], key=lambda x: x[0])\n",
    "max_shape = tuple([12992, 224])\n",
    "\n",
    "for key in test_image_data:\n",
    "    np_array = test_image_data[key]\n",
    "    padding = max_shape[0] - np_array.shape[0]\n",
    "    if padding > 0:\n",
    "        padding_shape = ((0, padding), (0, 0))\n",
    "        padded_np_array = np.pad(np_array, padding_shape, mode='constant', constant_values=0) # 0 is black (If I'm not mistaken)\n",
    "        test_image_data[key] = padded_np_array\n",
    "\n",
    "# Print the shapes of padded numpy arrays\n",
    "for key in test_image_data:\n",
    "    print(key, test_image_data[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked arrays for key 8785691: torch.Size([3, 12992, 224])\n"
     ]
    }
   ],
   "source": [
    "# Make 3D arrays\n",
    "\n",
    "# Group arrays by the first tuple values\n",
    "test_grouped_arrays = {}\n",
    "for key, value in test_image_data.items():\n",
    "    if key[0] not in test_grouped_arrays:\n",
    "        test_grouped_arrays[key[0]] = [value]\n",
    "    else:\n",
    "        test_grouped_arrays[key[0]].append(value)\n",
    "\n",
    "# Stack arrays with the same first tuple values into a 3D array\n",
    "test_stacked_arrays = {}\n",
    "for key, values in test_grouped_arrays.items():\n",
    "    test_stacked_arrays[key] = np.stack(values, axis=0)\n",
    "\n",
    "test_stacked_tensors = {}\n",
    "for key, value in test_stacked_arrays.items():\n",
    "    test_stacked_tensors[key] = torch.from_numpy(value)\n",
    "\n",
    "# Check the stacked arrays\n",
    "for key, value in test_stacked_tensors.items():\n",
    "    print(f\"Stacked arrays for key {key}: {value.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 12992, 224)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine the maximum shape\n",
    "# max_shape = tuple(max(arr.shape[i] for arr in test_stacked_tensors.values()) for i in range(3))\n",
    "max_shape = tuple([4, 12992, 224])\n",
    "\n",
    "# Pad each tensor to the maximum shape\n",
    "for key, tensor in test_stacked_tensors.items():\n",
    "    pad_shape = [max_shape[i] - tensor.shape[i] for i in range(3)]\n",
    "    test_stacked_tensors[key] = F.pad(tensor, (0, pad_shape[2], 0, pad_shape[1], 0, pad_shape[0]))\n",
    "max_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 12992, 224])\n"
     ]
    }
   ],
   "source": [
    "extracted_tensor = test_stacked_tensors['8785691']\n",
    "\n",
    "# Add a batch dimension\n",
    "predict_tensor = extracted_tensor.unsqueeze(0)\n",
    "print(predict_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0383, 0.0263, 0.0324],\n",
       "        [0.0402, 0.0385, 0.0323],\n",
       "        [0.0404, 0.0475, 0.0404],\n",
       "        [0.0402, 0.0270, 0.0667],\n",
       "        [0.0468, 0.0269, 0.0324],\n",
       "        [0.0468, 0.0270, 0.0324],\n",
       "        [0.0468, 0.0270, 0.0324],\n",
       "        [0.0384, 0.0270, 0.0324],\n",
       "        [0.0349, 0.0590, 0.0324],\n",
       "        [0.0355, 0.0545, 0.0501],\n",
       "        [0.0468, 0.0270, 0.0324],\n",
       "        [0.0469, 0.0270, 0.0324],\n",
       "        [0.0357, 0.0580, 0.0324],\n",
       "        [0.0331, 0.0542, 0.0323],\n",
       "        [0.0404, 0.0472, 0.0317],\n",
       "        [0.0468, 0.0270, 0.0324],\n",
       "        [0.0468, 0.0284, 0.0324],\n",
       "        [0.0404, 0.0472, 0.0324],\n",
       "        [0.0329, 0.0476, 0.0583],\n",
       "        [0.0368, 0.0528, 0.0324],\n",
       "        [0.0437, 0.0386, 0.0324],\n",
       "        [0.0404, 0.0377, 0.0586],\n",
       "        [0.0344, 0.0473, 0.0586],\n",
       "        [0.0304, 0.0521, 0.0585],\n",
       "        [0.0365, 0.0467, 0.0586]])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See the predictions\n",
    "predictions = predict(predict_tensor)\n",
    "# predictions = (predictions > 0.5).float()\n",
    "reshaped_tensor = predictions.view(25, 3)\n",
    "reshaped_tensor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
